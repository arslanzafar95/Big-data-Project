{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.types import DecimalType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt \n",
    "import geopandas\n",
    "import geoplot\n",
    "from pyproj import CRS\n",
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hello world "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"COVID\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "confirmed = spark.read.csv('time_series_19-covid-Confirmed_archived_0325.csv', header=True, inferSchema = True)\n",
    "\n",
    "# I will start by creating temporary table querying with SQL\n",
    "confirmed.createOrReplaceTempView('COVID_Confirmed')\n",
    "sparkConfirmedDF = spark.sql('''SELECT * FROM COVID_Confirmed''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Handle null values & Select only the relevant rows (i.e., those that include data about Australia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling (Dropping) 'NULL / None' Values\n",
    "ConfirmedDF = sparkConfirmedDF.na.drop()\n",
    "\n",
    "# Selecting Australia Confirmed Cases\n",
    "confirmed = spark.sql('''\n",
    "SELECT * FROM COVID_Confirmed \n",
    "WHERE `Country/Region` = 'Australia'\n",
    "''')\n",
    "confirmed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data in visualisation\n",
    "confirmed2=confirmed.toPandas()\n",
    "confirmed2.plot_bokeh(kind='bar', x='Province/State', y = list(confirmed2.columns.values),\n",
    "              xlabel='Provinces / States', ylabel='Frequency', \n",
    "              legend=False, title='Frequency by Days', figsize=(1000, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdf = geopandas.GeoDataFrame(confirmed2, geometry=geopandas.points_from_xy(confirmed2.Long, confirmed2.Lat))\n",
    "gpdf.crs = CRS.from_string(\"epsg:4326\")\n",
    "gpdf.plot_bokeh(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = confirmed.columns[14:]\n",
    "# Selecting a subset from the dataframe\n",
    "confirmed = confirmed.select(columns_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up all rows data into 1 row (merging)\n",
    "confirmed = confirmed.select(confirmed.columns).groupBy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merge the data to one row since we want to consider the entire Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sum up all rows data into 1 row (merging)\n",
    "confirmed = confirmed.select(confirmed.columns).groupBy().sum()\n",
    "\n",
    "# renaming column names for indexing\n",
    "counter = 0\n",
    "for column in confirmed.columns:\n",
    "    confirmed = confirmed.withColumnRenamed(f\"{column}\", f\"{counter}\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "# Converting to Pandas to transpose the dataframe\n",
    "confirmed_transpose = confirmed.toPandas().transpose()\n",
    "confirmed_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a Spark DataFrame from our Pandas DataFrame\n",
    "confirmedSparkDF = spark.createDataFrame(confirmed_transpose)\n",
    "confirmedSparkDF = confirmedSparkDF.withColumnRenamed(f\"{0}\", \"total_counts\")\n",
    "confirmedSparkDF = confirmedSparkDF.withColumn(\"indexes\", F.row_number()\n",
    "                                               .over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "confirmedSparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler = VectorAssembler(inputCols = ['indexes'], outputCol=\"Days\")\n",
    "output = featureassembler.transform(confirmedSparkDF)\n",
    "finalized_data = output.select(\"Days\", \"total_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Splitting into Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data.createOrReplaceTempView(\"Finalized_Cases\")\n",
    "train_data = spark.sql(\"SELECT * FROM Finalized_Cases LIMIT 50\")\n",
    "test_data = spark.sql(\"SELECT * FROM Finalized_Cases ORDER BY Days DESC LIMIT 2\")\n",
    "\n",
    "#####  Alternate method to split data into train and test data  ######\n",
    "# train_data = finalized_data.where(F.col('indexes') < 50)\n",
    "# test_data = finalized_data.where(F.col('indexes') > 49)\n",
    "\n",
    "train_data.show()\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the data\n",
    "#finalized_data.toPandas().drop(['Days'],axis=1).plot(kind='line',figsize=(10,5))\n",
    "finalized_data.toPandas().plot_bokeh(kind='line', legend=False, figsize=(1000, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol='Days', labelCol='total_counts', maxIter=10, regParam=0.8, elasticNetParam=0.5)\n",
    "regressor = regressor.fit(train_data)\n",
    "\n",
    "print('Regressor coefficient: ', regressor.coefficients)\n",
    "print('Regressor intercept: ', regressor.intercept)\n",
    "Summary = regressor.summary\n",
    "print(\"total Iterations: %d\" % Summary.totalIterations)\n",
    "print(\"RMSE: %f\" % Summary.rootMeanSquaredError)\n",
    "print(\"R-squared: %f\" % Summary.r2)\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)\n",
    "test_data = pred_results.predictions\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pred_results.predictions.select(\"prediction\").collect()\n",
    "test_data_actual = test_data.withColumnRenamed(\"total_counts\",\"actual\")\n",
    "test_data_actual = test_data_actual.select(\"actual\").collect()\n",
    "\n",
    "actual_cases = []\n",
    "predicted_cases = []\n",
    "for actual in test_data_actual:\n",
    "    actual_cases.append(float(actual.actual))  \n",
    "for predicts in prediction:\n",
    "    predicted_cases.append(predicts.prediction)\n",
    "    \n",
    "difference = [actual - predicted for actual, predicted in zip(actual_cases, predicted_cases)]\n",
    "differenceDF = spark.createDataFrame(difference, FloatType())\n",
    "differenceDF = differenceDF.withColumnRenamed(\"value\",\"difference_values\")\n",
    "differenceDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_DF = output.select(\"indexes\", \"total_counts\").withColumnRenamed(f\"indexes\", \"Days\")\n",
    "\n",
    "# Converting to Pandas to plot scatter and regression plots\n",
    "actual_data = actual_DF.toPandas()\n",
    "Predicted_df = test_data.toPandas()\n",
    "\n",
    "Predicted_df['Days']=[51,50]\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "sns.regplot(x='Days',y='total_counts',data=actual_data[:-2])\n",
    "sns.scatterplot(x='Days',y='total_counts',data=Predicted_df)\n",
    "sns.scatterplot(x='Days',y='prediction',data=Predicted_df,marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create residual plot\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.residplot(x='Days',y='total_counts',data=actual_data[:-2])\n",
    "sns.scatterplot(x=[51,50],y='difference_values',data=differenceDF.toPandas(),marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}